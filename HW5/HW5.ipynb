{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HW5: Evaluating machine translated output (31 possible points)\n",
        "\n",
        "In this homework, we will be assessing the properties of a machine translation system between a language you probably do not speak (Finnish) into a language you probably do speak (English). The goal of this homework is to get you to:\n",
        "\n",
        "1. Use the huggingface `transformers` package\n",
        "2. Build a function that can evaluate the quality of a back-translation\n",
        "3. Build a function that can evaluate machine translated output using a _monolingual_ language model\n",
        "\n",
        "## The data\n",
        "\n",
        "You will be using two files for this assignment -- `en-fi-en-translations.txt` and `fi-en-fi-translations.txt`. Each line in the files is tab-separated into columns. The following schema is used for both files:\n",
        "\n",
        "1. Language 1 (e.g. `ENG`)\n",
        "2. Language 2 (e.g. `FIN`)\n",
        "3. Correct intermediate sentence (e.g., the `FIN` translation)\n",
        "4. Original sentence (i.e., Language 1's original form)\n",
        "5. Round trip translated sentence (i.e., the output of translating from ENG -> FIN -> ENG)\n",
        "\n",
        "The data was generated using two large neural machine translation (NMT) models. These models were trained on open subtitle corpora, which are largely from movies and TV shows. NMT models are usually trained on parallel text, so the subtitles (or captions) from one segment in a movie are usually assumed to line up with the same line in another movie.\n",
        "\n",
        "Most of the modern models treat machine translation as a sequence-to-sequence problem. That is, we try to find the best representation of an input sequence (e.g., a sequence of English words) to predict an output sequence (e.g., a sequence of Finnish words). There are lots of tricks to make the model work, but for this homework we are interested in seeing how easily we can faithfully represent the original input to our machine translation systems by testing **backtranslation** or a specific case of round-trip machine translation that basically goes L1-L2-L1.\n",
        "\n",
        "## The languages\n",
        "\n",
        "But, languages vary in the way they encode different types of linguistic information. For example, Finnish has much more complex morphology than English does, which means that many different strings in Finnish can translate to exactly the same string in English. You can look up various grammatical properties of Finnish in the WALS database: https://wals.info/languoid/lect/wals_code_fin\n",
        "\n",
        "Here are a handful of facts about Finnish within the Morphology domain:\n",
        "\n",
        "* Exclusively concatenative\n",
        "* Case + number\n",
        "* 2-3 categories per word\n",
        "* Dependent marking\n",
        "* Double marking in possessive noun phrases\t\n",
        "* Strongly suffixing\n",
        "\n",
        "Likewise you can find out more about English here: https://wals.info/languoid/lect/wals_code_eng\n",
        "\n",
        "Take a look at these pages before you start the full assignment -- it will help you understand the data better."
      ],
      "metadata": {
        "id": "4KJTY7iGK9t9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1: Create a function that loads the data (2 points)\n",
        "\n",
        "**Create a function called `load_data`** that takes a file path, opens the file, processes the file using the `.readlines()` method. Create a list called `data`. Then the function should loop through each row and split it along the `\\t` character, and append this list to `data`. Return `data` at the end of the loop. "
      ],
      "metadata": {
        "id": "YJy3K-cqfgQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your answer for Q1 goes here\n",
        "\n"
      ],
      "metadata": {
        "id": "3XVOr1oVk5MR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2: Loading in data (4 points)"
      ],
      "metadata": {
        "id": "I_OFFhuZeF57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2a: Loading in the English round trip data (2 points)\n",
        "\n",
        "Load in `en-fi-en.txt` saved to a variable called `english`.\n",
        "\n",
        "You may load in the file to colab or jupyter however is most convenient for you.\n",
        "\n",
        "Print out the contents of `english`."
      ],
      "metadata": {
        "id": "jiYLkQ3RkkHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load in English data\n",
        "\n"
      ],
      "metadata": {
        "id": "Km3NimGXK-W_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print contents of `english`\n"
      ],
      "metadata": {
        "id": "-yvPdWh6twSK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2b: Loading in the Finnish round trip data (2 points)\n",
        "\n",
        "Load in `fi-en-fi.txt`, saved as a variable called `finnish`.\n",
        "\n",
        "You may load in the file to colab or jupyter however is most convenient for you.\n",
        "\n",
        "Print out the contents of `finnish`."
      ],
      "metadata": {
        "id": "Rhm7H3uth7zw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load in Finnish data\n"
      ],
      "metadata": {
        "id": "6fNCPJ9ni1GE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print contents of `finnish`\n",
        "\n"
      ],
      "metadata": {
        "id": "loJY23pDtxjC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3: Qualitative evaluation (7 points)\n",
        "\n",
        "Referring to the output of Q2a and Q2b, pick two cases that the model got right and three cases that the model got wrong, in terms of reconstructing the original message. Show us the 2 incorrect and 3 correct cases.\n",
        "\n",
        "For the Finnish translations, you may want to consider the English sentence as well. For both languages, propose 3 possible contributing _linguistic_ factors that may influence whether the backtranslations are correct/incorrect. For example, did any of the linguistic properties of English or Finnish from the WALS database appear in the examples you picked? \n",
        "\n",
        "As far as you can tell, does it look like one language was easier to translate into or out of than the other? Can you think of linguistic and non-linguistic reasons why this might be the case?"
      ],
      "metadata": {
        "id": "YWqjwxg-K9q1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q3a\n",
        "\n",
        "Right:\n",
        "1. \n",
        "> Sentences here \n",
        "2. \n",
        "> Sentences here\n",
        "\n",
        "Wrong:\n",
        "\n",
        "1. \n",
        "> Sentences here\n",
        "\n",
        "2. \n",
        "> Sentences here\n",
        "3. \n",
        "> Sentences here\n",
        "\n",
        "Qualitatively evaluate the English-English data here by answering the questions above."
      ],
      "metadata": {
        "id": "Wbz3qICWs6xu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q3b\n",
        "\n",
        "Right\n",
        "1. \n",
        "> Sentences here\n",
        "2.\n",
        "> Sentences here\n",
        "\n",
        "Wrong\n",
        "\n",
        "1. \n",
        "> Sentences here\n",
        "2.\n",
        "> Sentences here\n",
        "3.\n",
        "> Sentences here\n",
        "\n",
        "Qualitatively evaluate the Finnish-Finnish data here by answering the questions above."
      ],
      "metadata": {
        "id": "Bafz57yis-89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3c\n",
        "\n",
        "Discuss potential linguistic and non-linguistic reasons for errors."
      ],
      "metadata": {
        "id": "ccB2bRYrKgAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4: Exact string matching (5 points)\n",
        "\n",
        "For this question, you should compute the average accuracy of each of the datasets using _strict_ exact string matching. That is, you should compare the translated output (the 4th/final column in each data entry) to the gold output (the 3rd/second-to-last column in each data entry) for both `ENG` and `FIN`.\n",
        "\n",
        "To do this, write a function called `score` that does the following:\n",
        "\n",
        "0. Create an empty list called `matches`\n",
        "1. Takes the dataset (e.g., `english` or `finnish` as an argument) and loops through every row:\n",
        "  * Identifies the gold sentence\n",
        "  * Identifies the translated sentence\n",
        "  * Compares whether (2) is exactly the same string as (3) as a boolean called `match`\n",
        "  * Appends `match` to `matches`\n",
        "2. Returns the mean of `matches` (accuracy for the full dataset)\n",
        "\n",
        "For both `ENG` and `FIN`, run `score` and print the accuracy out to the notebook."
      ],
      "metadata": {
        "id": "1nn82Q__K9nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your `score` function here\n",
        "import numpy as np\n",
        "\n"
      ],
      "metadata": {
        "id": "WA3MrHNwK_Kw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute and print accuracy for english\n",
        "\n"
      ],
      "metadata": {
        "id": "XneLBcahLimI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute and print accuracy for finnish\n",
        "\n"
      ],
      "metadata": {
        "id": "o65PqmoyLicg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5: Monolingual embedding comparisons (10 points)\n",
        "\n",
        "In this section, we would like you to compute the similarity between the two sentences for each translation. Specifically, we will use `BERT`, a large language model, as a _sentence encoder_ that will produce one 768-dimensional vector for each token in our sentences. In the below, we use a tokenizer to transform a sentence $s$ into $k$ subwords, and then we give a pre-trained model the tokenized representation of $s$. The result is a `(1, k, 768)` dimensional tensor at each layer.\n",
        "\n",
        "The layers are held together in a `tuple`, in which the first element is the lowest layer, and the last element is the highest layer."
      ],
      "metadata": {
        "id": "aVhO6TXxK9f-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5a: Run the next cell to load in the English and Finnish BERT models (1 points)"
      ],
      "metadata": {
        "id": "Uex069kuOMh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from torch.nn.functional import cosine_similarity\n",
        "\n",
        "finbert = BertModel.from_pretrained(\"TurkuNLP/bert-base-finnish-cased-v1\")\n",
        "finbert.eval()\n",
        "fintokenizer = BertTokenizer.from_pretrained(\"TurkuNLP/bert-base-finnish-cased-v1\")\n",
        "\n",
        "engbert = BertModel.from_pretrained(\"bert-base-cased\")\n",
        "engbert.eval()\n",
        "engtokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "_LYnRPGkOL7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5b: Computing the similarity between two English sentences at a specific layer (3 points)\n",
        "\n",
        "Note, you do NOT need to change `embed_sentence` or `sentence_similarity`. Changing them will cause you to lose points for this question."
      ],
      "metadata": {
        "id": "IDpOh_cUPutg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_sentence(model, tokenizer, s):\n",
        "  s_tokenized = tokenizer(s, return_tensors=\"pt\")\n",
        "  s_embeds = model(**s_tokenized, output_hidden_states=True)['hidden_states']\n",
        "  return s_embeds\n",
        "\n",
        "\n",
        "def embed_sentences(model, tokenizer, s1, s2):\n",
        "  s1_embeds = embed_sentence(model, tokenizer, s1)\n",
        "  s2_embeds = embed_sentence(model, tokenizer, s2)\n",
        "  return s1_embeds, s2_embeds\n",
        "\n",
        "\n",
        "def sentence_similarity(s1_embeds, s2_embeds, layer):\n",
        "  s1_vector = convert_embeds_to_vector(s1_embeds, layer)\n",
        "  s2_vector = convert_embeds_to_vector(s2_embeds, layer)\n",
        "  similarity = cosine_similarity(s1_vector, s2_vector).detach().item()\n",
        "  return similarity"
      ],
      "metadata": {
        "id": "_dqq1GaDmPOw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClFHk940K4_J"
      },
      "outputs": [],
      "source": [
        "def convert_embeds_to_vector(embeds, layer):\n",
        "  # TODO: get sentence embeddings at a specific layer\n",
        "\n",
        "  # TODO: compute the mean along axis=1 to turn the embeddings into a single vector of size (1, 768)\n",
        "\n",
        "  # TODO: verify the shape of the vector is (1, 768) using assert\n",
        "\n",
        "  return vector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After editing the above code, run the cell below. The output you should get is that sentence 1 and sentence 2 have cosine similarity $\\approx 0.95$."
      ],
      "metadata": {
        "id": "G3SLt_puTb6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent1 = 'My cat Vector is very cute.'\n",
        "sent2 = 'Professor Jacobs has a cat named Vector who is very cute.'\n",
        "\n",
        "s1_embeds, s2_embeds = embed_sentences(engbert, engtokenizer, sent1, sent2)\n",
        "print(sentence_similarity(s1_embeds, s2_embeds, layer=6))"
      ],
      "metadata": {
        "id": "-Eb7nsrXTekh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5c: Computing the similarity between all English gold and translated sentences at all layers (3 points)\n",
        "\n",
        "Create a nested `for` loop to iterate through all the sentences in `english` using `sentence_similarity` from Q5b.\n",
        "\n",
        "Outer loop -- all sentences\n",
        "\n",
        "Inner loop -- all layers\n",
        "\n",
        "Print averages of all layers out at the end"
      ],
      "metadata": {
        "id": "iBbXnyGzQItf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "layer_sims = defaultdict(list)\n",
        "\n"
      ],
      "metadata": {
        "id": "YwrO4qgEVxu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loop over each layer in layer_sims and print the average"
      ],
      "metadata": {
        "id": "5nQwZGnbI7BK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5d: Compute the similarity between all Finnish gold and translated sentences at all layers (3 points)\n",
        "\n",
        "Create a `for` loop to iterate through all the sentences in `finnish` using `sentence_similarity` from Q5b.\n",
        "\n",
        "Outer loop -- all sentences\n",
        "\n",
        "Inner loop -- all layers\n",
        "\n",
        "Print averages of all layers out at the end"
      ],
      "metadata": {
        "id": "gazIju2AQyEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer_sims = defaultdict(list)\n"
      ],
      "metadata": {
        "id": "BkROJveRV7II"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loop over each layer in layer_sims and print the average"
      ],
      "metadata": {
        "id": "jxuqUH0dOfgz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q6: Compare and contrast exact matches to embedding distances (3 points)\n",
        "\n",
        "Exact matches and embedding distances can be thought of as two opposite ends of a spectrum. Write down 1 clear advantage that you see for each method over the other one (1 point each). For example, what information does an an embedding distance give you over an exact string match, and vice versa? Can you think of specific cases in the above analyses where neither method is very good? (1 point)"
      ],
      "metadata": {
        "id": "h66qx0pydxk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your free response to Q6 goes here."
      ],
      "metadata": {
        "id": "znhQijZzd2qf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus: Compute string similarity between gold and machine translated sentences (5 points)\n",
        "\n",
        "There are a number of ways two strings can be similar but different. Consider any **ONE** (1) of the following measures for computing the **mean** similarity between the gold sentence and the machine translated sentence. Summarize why that measure would be appropriate for comparing the input and output strings. Consider discussion of these points throughout the SLP3 book as well as Wikipedia as well as [Manning, Wein, and Schneider (2020)](https://aclanthology.org/2020.coling-main.420.pdf). You may use any off-the-shelf implementation that you find as long as you cite it below.\n",
        "\n",
        "* [BLEU](https://en.wikipedia.org/wiki/BLEU)\n",
        "* [ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric))\n",
        "* [Levenstein Distance](https://en.wikipedia.org/wiki/Levenshtein_distance)\n",
        "\n",
        "For full points, this question must:\n",
        "\n",
        "1. Justify the metric (NO quoting from Wikipedia)\n",
        "2. Summarize the results for English AND Finnish\n",
        "3. Cite all sources"
      ],
      "metadata": {
        "id": "Td6WLaeNrqTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bonus code here"
      ],
      "metadata": {
        "id": "XFyy28rqLz93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bonus free response"
      ],
      "metadata": {
        "id": "IEsooKjYM5C8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission format (1 point)\n",
        "\n",
        "Please upload the file with the name\n",
        "\n",
        "LASTNAME_FIRSTNAME_HW5.ipynb"
      ],
      "metadata": {
        "id": "ReECQdyfiJfK"
      }
    }
  ]
}