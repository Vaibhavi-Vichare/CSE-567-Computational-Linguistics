{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccD5Kq24WAN3"
      },
      "source": [
        "# Question 1 (5 points)\n",
        "Load in all of the packages you will need for this assignment in the cell below. \n",
        "\n",
        "If you load in other packages later in the notebook, be sure to bring them up here. This is good coding practice and will look cleaner for everyone when reading your code.\n",
        "\n",
        "You will need the following:\n",
        "\n",
        "* To load a plain text file (`abstracts.tsv`) in with the colab interface (either local to your drive or by uploading the file to the notebook)\n",
        "* The NLTK tokenizer for English\n",
        "* The spaCy word tokenizer for English"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfOQBQztFnqW",
        "outputId": "c69dad68-f987-4b0a-9c0e-378db8260bf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting iteration_utilities\n",
            "  Downloading iteration_utilities-0.11.0-cp37-cp37m-manylinux2014_x86_64.whl (283 kB)\n",
            "\u001b[K     |████████████████████████████████| 283 kB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: iteration-utilities\n",
            "Successfully installed iteration-utilities-0.11.0\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "# Load in packages that you will use in this notebook\n",
        "! pip install iteration_utilities\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import spacy\n",
        "import itertools\n",
        "\n",
        "from pprint import pprint\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from nltk import word_tokenize\n",
        "from collections import Counter\n",
        "from iteration_utilities import Iterable\n",
        "from itertools import chain\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "\n",
        "\n",
        "# put other packages you will use below this line\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZSPSo-TWTxL"
      },
      "source": [
        "# Question 2 (1 point)\n",
        "\n",
        "Load in the file called `abstracts.tsv` in the `data/` subdirectory of this folder into this notebook.\n",
        "\n",
        "Uncomment one of the two blocks below.\n",
        "\n",
        "Then, edit the line that you uncommented to load in abstracts.tsv.\n",
        "\n",
        "Note that using the `files` command requires you to do a bit more work to load the file in in Question 3. Be sure to check previous notebooks.\n",
        "\n",
        "If you do this in Jupyter on your own machine, please load in the file in the same manner without these imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "gKNdusogie9p",
        "outputId": "64560d88-c3aa-4e2d-9b91-2ec903dc357d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-13ba94fd-e79a-4c19-84ea-5b645a8b15cc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-13ba94fd-e79a-4c19-84ea-5b645a8b15cc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving abstracts.tsv to abstracts.tsv\n"
          ]
        }
      ],
      "source": [
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9FX1mY1L3-1S"
      },
      "outputs": [],
      "source": [
        "#abstracts = uploaded['abstracts.tsv'].decode('utf-8')\n",
        "with open ('abstracts.tsv', 'r') as handle:\n",
        "  abstracts = handle.read().split('\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b00_5_49AE5B"
      },
      "outputs": [],
      "source": [
        "abstracts[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRL5OkT4Wn-4"
      },
      "source": [
        "# Question 3: 3 points\n",
        "\n",
        "In this section, we will be comparing different preprocessing strategies. For this question, you should first preview the data by looking at the first 5 lines. Use [a slice](https://stackoverflow.com/questions/509211/understanding-slice-notation) to print the first five elements from the array.\n",
        "\n",
        "Then, separate all of the abstracts on all whitespace. Store this in an array of string arrays called `split_abstracts`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nq6-78b1IGPm"
      },
      "outputs": [],
      "source": [
        "# preview data (print the first five lines)\n",
        "abstracts[0:5]\n",
        "# split every sentence on whitespace and save array\n",
        "split_abstract = []\n",
        "for abstract in abstracts:\n",
        "  split_abstract.append(abstract.split())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nhn1-2we6HR5"
      },
      "outputs": [],
      "source": [
        "split_abstract[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UqHOFeVIGzB"
      },
      "source": [
        "# Question 4: 4 points\n",
        "\n",
        "Now, we are going to use the `nltk` `word_tokenize` function. You should have loaded this above in the very first block. Use `word_tokenize` on all the abstracts and store this in an array of string arrays called `nltk_tokenized_abstracts`. Use a slice to print the fifth to the tenth elements of the array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Gig07Ml_-poo"
      },
      "outputs": [],
      "source": [
        "# use nltk's word_tokenize function over all of the abstracts\n",
        "nltk_tokenized_abstracts = []\n",
        "\n",
        "for abstract in abstracts:\n",
        "  nltk_tokenized_abstracts.append(word_tokenize(abstract))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ytn9RUyx_fUV"
      },
      "outputs": [],
      "source": [
        "nltk_tokenized_abstracts[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PpfhdWSI0Me"
      },
      "source": [
        "# Question 5: 5 points\n",
        "\n",
        "Now, we are going to use the `spacy` tokenization function. The output that spacy gives you is more complicated than the output of `nltk`'s `word_tokenize` function, because the `spacy` API takes a string (e.g., \"I like cheese\") and returns a `Doc` object. Within the `Doc` object there are `Token`s, and each `Token` has a `text` object. \n",
        "\n",
        "For this question, what you need to do is implement another loop through all of the abstracts, and store a list (array) of all of the token _strings_ from each `Token` object. If you were paying attention during the tokenization lecture this should be easy.\n",
        "\n",
        "Store all of these tokenizations into an array of string arrays called `spacy_tokenized_abstracts`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_model = spacy.load('en_core_web_sm')\n",
        "tokenizer = spacy_model.tokenizer\n",
        "\n",
        "# save the output into a variable\n",
        "spacy_tokenized_abstract = []\n",
        "for iterate_abs in abstracts:\n",
        "  spacy_abstract = tokenizer(iterate_abs)\n",
        "  temp = []\n",
        "  for words in spacy_abstract:\n",
        "    temp.append(words.text)\n",
        "  spacy_tokenized_abstract.append(temp)\n"
      ],
      "metadata": {
        "id": "z9pvSkOOPKKH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "spacy_tokenized_abstract[0:5]"
      ],
      "metadata": {
        "id": "UC05PWNSmmqn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43cb2a73-cd5e-45c9-fbbc-1f06c19b4eff"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Offensive',\n",
              "  'language',\n",
              "  'detection',\n",
              "  '(',\n",
              "  'OLD',\n",
              "  ')',\n",
              "  'has',\n",
              "  'received',\n",
              "  'increasing',\n",
              "  'attention',\n",
              "  'due',\n",
              "  'to',\n",
              "  'its',\n",
              "  'societal',\n",
              "  'impact',\n",
              "  '.',\n",
              "  'Recent',\n",
              "  'work',\n",
              "  'shows',\n",
              "  'that',\n",
              "  'bidirectional',\n",
              "  'transformer',\n",
              "  'based',\n",
              "  'methods',\n",
              "  'obtain',\n",
              "  'impressive',\n",
              "  'performance',\n",
              "  'on',\n",
              "  'OLD',\n",
              "  '.',\n",
              "  'However',\n",
              "  ',',\n",
              "  'such',\n",
              "  'methods',\n",
              "  'usually',\n",
              "  'rely',\n",
              "  'on',\n",
              "  'large',\n",
              "  '-',\n",
              "  'scale',\n",
              "  'well',\n",
              "  '-',\n",
              "  'labeled',\n",
              "  'OLD',\n",
              "  'datasets',\n",
              "  'for',\n",
              "  'model',\n",
              "  'training',\n",
              "  '.',\n",
              "  'To',\n",
              "  'address',\n",
              "  'the',\n",
              "  'issue',\n",
              "  'of',\n",
              "  'data',\n",
              "  '/',\n",
              "  'label',\n",
              "  'scarcity',\n",
              "  'in',\n",
              "  'OLD',\n",
              "  ',',\n",
              "  'in',\n",
              "  'this',\n",
              "  'paper',\n",
              "  ',',\n",
              "  'we',\n",
              "  'propose',\n",
              "  'a',\n",
              "  'simple',\n",
              "  'yet',\n",
              "  'effective',\n",
              "  'domain',\n",
              "  'adaptation',\n",
              "  'approach',\n",
              "  'to',\n",
              "  'train',\n",
              "  'bidirectional',\n",
              "  'transformers',\n",
              "  '.',\n",
              "  'Our',\n",
              "  'approach',\n",
              "  'introduces',\n",
              "  'domain',\n",
              "  'adaptation',\n",
              "  '(',\n",
              "  'DA',\n",
              "  ')',\n",
              "  'training',\n",
              "  'procedures',\n",
              "  'to',\n",
              "  'ALBERT',\n",
              "  ',',\n",
              "  'such',\n",
              "  'that',\n",
              "  'it',\n",
              "  'can',\n",
              "  'effectively',\n",
              "  'exploit',\n",
              "  'auxiliary',\n",
              "  'data',\n",
              "  'from',\n",
              "  'source',\n",
              "  'domains',\n",
              "  'to',\n",
              "  'improve',\n",
              "  'the',\n",
              "  'OLD',\n",
              "  'performance',\n",
              "  'in',\n",
              "  'a',\n",
              "  'target',\n",
              "  'domain',\n",
              "  '.',\n",
              "  'Experimental',\n",
              "  'results',\n",
              "  'on',\n",
              "  'benchmark',\n",
              "  'datasets',\n",
              "  'show',\n",
              "  'that',\n",
              "  'our',\n",
              "  'approach',\n",
              "  ',',\n",
              "  'ALBERT',\n",
              "  '(',\n",
              "  'DA',\n",
              "  ')',\n",
              "  ',',\n",
              "  'obtains',\n",
              "  'the',\n",
              "  'state',\n",
              "  '-',\n",
              "  'of',\n",
              "  '-',\n",
              "  'the',\n",
              "  '-',\n",
              "  'art',\n",
              "  'performance',\n",
              "  'in',\n",
              "  'most',\n",
              "  'cases',\n",
              "  '.',\n",
              "  'Particularly',\n",
              "  ',',\n",
              "  'our',\n",
              "  'approach',\n",
              "  'significantly',\n",
              "  'benefits',\n",
              "  'underrepresented',\n",
              "  'and',\n",
              "  'under',\n",
              "  '-',\n",
              "  'performing',\n",
              "  'classes',\n",
              "  ',',\n",
              "  'with',\n",
              "  'a',\n",
              "  'significant',\n",
              "  'improvement',\n",
              "  'over',\n",
              "  'ALBERT',\n",
              "  '.'],\n",
              " ['Hate',\n",
              "  'speech',\n",
              "  'and',\n",
              "  'profanity',\n",
              "  'detection',\n",
              "  'suffer',\n",
              "  'from',\n",
              "  'data',\n",
              "  'sparsity',\n",
              "  ',',\n",
              "  'especially',\n",
              "  'for',\n",
              "  'languages',\n",
              "  'other',\n",
              "  'than',\n",
              "  'English',\n",
              "  ',',\n",
              "  'due',\n",
              "  'to',\n",
              "  'the',\n",
              "  'subjective',\n",
              "  'nature',\n",
              "  'of',\n",
              "  'the',\n",
              "  'tasks',\n",
              "  'and',\n",
              "  'the',\n",
              "  'resulting',\n",
              "  'annotation',\n",
              "  'incompatibility',\n",
              "  'of',\n",
              "  'existing',\n",
              "  'corpora',\n",
              "  '.',\n",
              "  'In',\n",
              "  'this',\n",
              "  'study',\n",
              "  ',',\n",
              "  'we',\n",
              "  'identify',\n",
              "  'profane',\n",
              "  'subspaces',\n",
              "  'in',\n",
              "  'word',\n",
              "  'and',\n",
              "  'sentence',\n",
              "  'representations',\n",
              "  'and',\n",
              "  'explore',\n",
              "  'their',\n",
              "  'generalization',\n",
              "  'capability',\n",
              "  'on',\n",
              "  'a',\n",
              "  'variety',\n",
              "  'of',\n",
              "  'similar',\n",
              "  'and',\n",
              "  'distant',\n",
              "  'target',\n",
              "  'tasks',\n",
              "  'in',\n",
              "  'a',\n",
              "  'zero',\n",
              "  '-',\n",
              "  'shot',\n",
              "  'setting',\n",
              "  '.',\n",
              "  'This',\n",
              "  'is',\n",
              "  'done',\n",
              "  'monolingually',\n",
              "  '(',\n",
              "  'German',\n",
              "  ')',\n",
              "  'and',\n",
              "  'cross',\n",
              "  '-',\n",
              "  'lingually',\n",
              "  'to',\n",
              "  'closely',\n",
              "  '-',\n",
              "  'related',\n",
              "  '(',\n",
              "  'English',\n",
              "  ')',\n",
              "  ',',\n",
              "  'distantly',\n",
              "  '-',\n",
              "  'related',\n",
              "  '(',\n",
              "  'French',\n",
              "  ')',\n",
              "  'and',\n",
              "  'non',\n",
              "  '-',\n",
              "  'related',\n",
              "  '(',\n",
              "  'Arabic',\n",
              "  ')',\n",
              "  'tasks',\n",
              "  '.',\n",
              "  'We',\n",
              "  'observe',\n",
              "  'that',\n",
              "  ',',\n",
              "  'on',\n",
              "  'both',\n",
              "  'similar',\n",
              "  'and',\n",
              "  'distant',\n",
              "  'target',\n",
              "  'tasks',\n",
              "  'and',\n",
              "  'across',\n",
              "  'all',\n",
              "  'languages',\n",
              "  ',',\n",
              "  'the',\n",
              "  'subspace',\n",
              "  '-',\n",
              "  'based',\n",
              "  'representations',\n",
              "  'transfer',\n",
              "  'more',\n",
              "  'effectively',\n",
              "  'than',\n",
              "  'standard',\n",
              "  'BERT',\n",
              "  'representations',\n",
              "  'in',\n",
              "  'the',\n",
              "  'zero',\n",
              "  '-',\n",
              "  'shot',\n",
              "  'setting',\n",
              "  ',',\n",
              "  'with',\n",
              "  'improvements',\n",
              "  'between',\n",
              "  'F1',\n",
              "  '+10.9',\n",
              "  'and',\n",
              "  'F1',\n",
              "  '+42.9',\n",
              "  'over',\n",
              "  'the',\n",
              "  'baselines',\n",
              "  'across',\n",
              "  'all',\n",
              "  'tested',\n",
              "  'monolingual',\n",
              "  'and',\n",
              "  'cross',\n",
              "  '-',\n",
              "  'lingual',\n",
              "  'scenarios',\n",
              "  '.'],\n",
              " ['We',\n",
              "  'introduce',\n",
              "  'HateBERT',\n",
              "  ',',\n",
              "  'a',\n",
              "  're',\n",
              "  '-',\n",
              "  'trained',\n",
              "  'BERT',\n",
              "  'model',\n",
              "  'for',\n",
              "  'abusive',\n",
              "  'language',\n",
              "  'detection',\n",
              "  'in',\n",
              "  'English',\n",
              "  '.',\n",
              "  'The',\n",
              "  'model',\n",
              "  'was',\n",
              "  'trained',\n",
              "  'on',\n",
              "  'RAL',\n",
              "  '-',\n",
              "  'E',\n",
              "  ',',\n",
              "  'a',\n",
              "  'large',\n",
              "  '-',\n",
              "  'scale',\n",
              "  'dataset',\n",
              "  'of',\n",
              "  'Reddit',\n",
              "  'comments',\n",
              "  'in',\n",
              "  'English',\n",
              "  'from',\n",
              "  'communities',\n",
              "  'banned',\n",
              "  'for',\n",
              "  'being',\n",
              "  'offensive',\n",
              "  ',',\n",
              "  'abusive',\n",
              "  ',',\n",
              "  'or',\n",
              "  'hateful',\n",
              "  'that',\n",
              "  'we',\n",
              "  'have',\n",
              "  'curated',\n",
              "  'and',\n",
              "  'made',\n",
              "  'available',\n",
              "  'to',\n",
              "  'the',\n",
              "  'public',\n",
              "  '.',\n",
              "  'We',\n",
              "  'present',\n",
              "  'the',\n",
              "  'results',\n",
              "  'of',\n",
              "  'a',\n",
              "  'detailed',\n",
              "  'comparison',\n",
              "  'between',\n",
              "  'a',\n",
              "  'general',\n",
              "  'pre',\n",
              "  '-',\n",
              "  'trained',\n",
              "  'language',\n",
              "  'model',\n",
              "  'and',\n",
              "  'the',\n",
              "  'retrained',\n",
              "  'version',\n",
              "  'on',\n",
              "  'three',\n",
              "  'English',\n",
              "  'datasets',\n",
              "  'for',\n",
              "  'offensive',\n",
              "  ',',\n",
              "  'abusive',\n",
              "  'language',\n",
              "  'and',\n",
              "  'hate',\n",
              "  'speech',\n",
              "  'detection',\n",
              "  'tasks',\n",
              "  '.',\n",
              "  'In',\n",
              "  'all',\n",
              "  'datasets',\n",
              "  ',',\n",
              "  'HateBERT',\n",
              "  'outperforms',\n",
              "  'the',\n",
              "  'corresponding',\n",
              "  'general',\n",
              "  'BERT',\n",
              "  'model',\n",
              "  '.',\n",
              "  'We',\n",
              "  'also',\n",
              "  'discuss',\n",
              "  'a',\n",
              "  'battery',\n",
              "  'of',\n",
              "  'experiments',\n",
              "  'comparing',\n",
              "  'the',\n",
              "  'portability',\n",
              "  'of',\n",
              "  'the',\n",
              "  'fine',\n",
              "  '-',\n",
              "  'tuned',\n",
              "  'models',\n",
              "  'across',\n",
              "  'the',\n",
              "  'datasets',\n",
              "  ',',\n",
              "  'suggesting',\n",
              "  'that',\n",
              "  'portability',\n",
              "  'is',\n",
              "  'affected',\n",
              "  'by',\n",
              "  'compatibility',\n",
              "  'of',\n",
              "  'the',\n",
              "  'annotated',\n",
              "  'phenomena',\n",
              "  '.'],\n",
              " ['Hateful',\n",
              "  'memes',\n",
              "  'pose',\n",
              "  'a',\n",
              "  'unique',\n",
              "  'challenge',\n",
              "  'for',\n",
              "  'current',\n",
              "  'machine',\n",
              "  'learning',\n",
              "  'systems',\n",
              "  'because',\n",
              "  'their',\n",
              "  'message',\n",
              "  'is',\n",
              "  'derived',\n",
              "  'from',\n",
              "  'both',\n",
              "  'text-',\n",
              "  'and',\n",
              "  'visual',\n",
              "  '-',\n",
              "  'modalities',\n",
              "  '.',\n",
              "  'To',\n",
              "  'this',\n",
              "  'effect',\n",
              "  ',',\n",
              "  'Facebook',\n",
              "  'released',\n",
              "  'the',\n",
              "  'Hateful',\n",
              "  'Memes',\n",
              "  'Challenge',\n",
              "  ',',\n",
              "  'a',\n",
              "  'dataset',\n",
              "  'of',\n",
              "  'memes',\n",
              "  'with',\n",
              "  'pre',\n",
              "  '-',\n",
              "  'extracted',\n",
              "  'text',\n",
              "  'captions',\n",
              "  ',',\n",
              "  'but',\n",
              "  'it',\n",
              "  'is',\n",
              "  'unclear',\n",
              "  'whether',\n",
              "  'these',\n",
              "  'synthetic',\n",
              "  'examples',\n",
              "  'generalize',\n",
              "  'to',\n",
              "  '{',\n",
              "  '`',\n",
              "  '}',\n",
              "  'memes',\n",
              "  'in',\n",
              "  'the',\n",
              "  'wild',\n",
              "  '{',\n",
              "  \"'\",\n",
              "  '}',\n",
              "  '.',\n",
              "  'In',\n",
              "  'this',\n",
              "  'paper',\n",
              "  ',',\n",
              "  'we',\n",
              "  'collect',\n",
              "  'hateful',\n",
              "  'and',\n",
              "  'non',\n",
              "  '-',\n",
              "  'hateful',\n",
              "  'memes',\n",
              "  'from',\n",
              "  'Pinterest',\n",
              "  'to',\n",
              "  'evaluate',\n",
              "  'out',\n",
              "  '-',\n",
              "  'of',\n",
              "  '-',\n",
              "  'sample',\n",
              "  'performance',\n",
              "  'on',\n",
              "  'models',\n",
              "  'pre',\n",
              "  '-',\n",
              "  'trained',\n",
              "  'on',\n",
              "  'the',\n",
              "  'Facebook',\n",
              "  'dataset',\n",
              "  '.',\n",
              "  'We',\n",
              "  'find',\n",
              "  'that',\n",
              "  '{',\n",
              "  '`',\n",
              "  '}',\n",
              "  'memes',\n",
              "  'in',\n",
              "  'the',\n",
              "  'wild',\n",
              "  '{',\n",
              "  \"'\",\n",
              "  '}',\n",
              "  'differ',\n",
              "  'in',\n",
              "  'two',\n",
              "  'key',\n",
              "  'aspects',\n",
              "  ':',\n",
              "  '1',\n",
              "  ')',\n",
              "  'Captions',\n",
              "  'must',\n",
              "  'be',\n",
              "  'extracted',\n",
              "  'via',\n",
              "  'OCR',\n",
              "  ',',\n",
              "  'injecting',\n",
              "  'noise',\n",
              "  'and',\n",
              "  'diminishing',\n",
              "  'performance',\n",
              "  'of',\n",
              "  'multimodal',\n",
              "  'models',\n",
              "  ',',\n",
              "  'and',\n",
              "  '2',\n",
              "  ')',\n",
              "  'Memes',\n",
              "  'are',\n",
              "  'more',\n",
              "  'diverse',\n",
              "  'than',\n",
              "  '{',\n",
              "  '`',\n",
              "  '}',\n",
              "  'traditional',\n",
              "  'memes',\n",
              "  '{',\n",
              "  \"'\",\n",
              "  '}',\n",
              "  ',',\n",
              "  'including',\n",
              "  'screenshots',\n",
              "  'of',\n",
              "  'conversations',\n",
              "  'or',\n",
              "  'text',\n",
              "  'on',\n",
              "  'a',\n",
              "  'plain',\n",
              "  'background',\n",
              "  '.',\n",
              "  'This',\n",
              "  'paper',\n",
              "  'thus',\n",
              "  'serves',\n",
              "  'as',\n",
              "  'a',\n",
              "  'reality',\n",
              "  '-',\n",
              "  'check',\n",
              "  'for',\n",
              "  'the',\n",
              "  'current',\n",
              "  'benchmark',\n",
              "  'of',\n",
              "  'hateful',\n",
              "  'meme',\n",
              "  'detection',\n",
              "  'and',\n",
              "  'its',\n",
              "  'applicability',\n",
              "  'for',\n",
              "  'detecting',\n",
              "  'real',\n",
              "  'world',\n",
              "  'hate',\n",
              "  '.'],\n",
              " ['Content',\n",
              "  'moderation',\n",
              "  'is',\n",
              "  'often',\n",
              "  'performed',\n",
              "  'by',\n",
              "  'a',\n",
              "  'collaboration',\n",
              "  'between',\n",
              "  'humans',\n",
              "  'and',\n",
              "  'machine',\n",
              "  'learning',\n",
              "  'models',\n",
              "  '.',\n",
              "  'However',\n",
              "  ',',\n",
              "  'it',\n",
              "  'is',\n",
              "  'not',\n",
              "  'well',\n",
              "  'understood',\n",
              "  'how',\n",
              "  'to',\n",
              "  'design',\n",
              "  'the',\n",
              "  'collaborative',\n",
              "  'process',\n",
              "  'so',\n",
              "  'as',\n",
              "  'to',\n",
              "  'maximize',\n",
              "  'the',\n",
              "  'combined',\n",
              "  'moderator',\n",
              "  '-',\n",
              "  'model',\n",
              "  'system',\n",
              "  'performance',\n",
              "  '.',\n",
              "  'This',\n",
              "  'work',\n",
              "  'presents',\n",
              "  'a',\n",
              "  'rigorous',\n",
              "  'study',\n",
              "  'of',\n",
              "  'this',\n",
              "  'problem',\n",
              "  ',',\n",
              "  'focusing',\n",
              "  'on',\n",
              "  'an',\n",
              "  'approach',\n",
              "  'that',\n",
              "  'incorporates',\n",
              "  'model',\n",
              "  'uncertainty',\n",
              "  'into',\n",
              "  'the',\n",
              "  'collaborative',\n",
              "  'process',\n",
              "  '.',\n",
              "  'First',\n",
              "  ',',\n",
              "  'we',\n",
              "  'introduce',\n",
              "  'principled',\n",
              "  'metrics',\n",
              "  'to',\n",
              "  'describe',\n",
              "  'the',\n",
              "  'performance',\n",
              "  'of',\n",
              "  'the',\n",
              "  'collaborative',\n",
              "  'system',\n",
              "  'under',\n",
              "  'capacity',\n",
              "  'constraints',\n",
              "  'on',\n",
              "  'the',\n",
              "  'human',\n",
              "  'moderator',\n",
              "  ',',\n",
              "  'quantifying',\n",
              "  'how',\n",
              "  'efficiently',\n",
              "  'the',\n",
              "  'combined',\n",
              "  'system',\n",
              "  'utilizes',\n",
              "  'human',\n",
              "  'decisions',\n",
              "  '.',\n",
              "  'Using',\n",
              "  'these',\n",
              "  'metrics',\n",
              "  ',',\n",
              "  'we',\n",
              "  'conduct',\n",
              "  'a',\n",
              "  'large',\n",
              "  'benchmark',\n",
              "  'study',\n",
              "  'evaluating',\n",
              "  'the',\n",
              "  'performance',\n",
              "  'of',\n",
              "  'state',\n",
              "  '-',\n",
              "  'of',\n",
              "  '-',\n",
              "  'the',\n",
              "  '-',\n",
              "  'art',\n",
              "  'uncertainty',\n",
              "  'models',\n",
              "  'under',\n",
              "  'different',\n",
              "  'collaborative',\n",
              "  'review',\n",
              "  'strategies',\n",
              "  '.',\n",
              "  'We',\n",
              "  'find',\n",
              "  'that',\n",
              "  'an',\n",
              "  'uncertainty',\n",
              "  '-',\n",
              "  'based',\n",
              "  'strategy',\n",
              "  'consistently',\n",
              "  'outperforms',\n",
              "  'the',\n",
              "  'widely',\n",
              "  'used',\n",
              "  'strategy',\n",
              "  'based',\n",
              "  'on',\n",
              "  'toxicity',\n",
              "  'scores',\n",
              "  ',',\n",
              "  'and',\n",
              "  'moreover',\n",
              "  'that',\n",
              "  'the',\n",
              "  'choice',\n",
              "  'of',\n",
              "  'review',\n",
              "  'strategy',\n",
              "  'drastically',\n",
              "  'changes',\n",
              "  'the',\n",
              "  'overall',\n",
              "  'system',\n",
              "  'performance',\n",
              "  '.',\n",
              "  'Our',\n",
              "  'results',\n",
              "  'demonstrate',\n",
              "  'the',\n",
              "  'importance',\n",
              "  'of',\n",
              "  'rigorous',\n",
              "  'metrics',\n",
              "  'for',\n",
              "  'understanding',\n",
              "  'and',\n",
              "  'developing',\n",
              "  'effective',\n",
              "  'moderator',\n",
              "  '-',\n",
              "  'model',\n",
              "  'systems',\n",
              "  'for',\n",
              "  'content',\n",
              "  'moderation',\n",
              "  ',',\n",
              "  'as',\n",
              "  'well',\n",
              "  'as',\n",
              "  'the',\n",
              "  'utility',\n",
              "  'of',\n",
              "  'uncertainty',\n",
              "  'estimation',\n",
              "  'in',\n",
              "  'this',\n",
              "  'domain',\n",
              "  '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLC2daI-Myn1"
      },
      "source": [
        "# Question 6: Compare tokenizations (8 points)\n",
        "\n",
        "Now that we have three tokenizations (`split_abstracts`, `nltk_tokenized_abstracts`, and `spacy_tokenized_abstracts`), we want to compare how similar the tokenizations are. Pick a slice of 5 abstracts with any start and end indices. Demonstrate that the total number of abstracts that you selected is 5 by printing the length of that subset of abstracts.\n",
        "\n",
        "Tokenize each of the 5 abstracts according to each of the three approach above, and print their output in the code cell below. Then, in the cell below that, explain how these tokenizations differ. What are the strengths and weaknesses of each tokenization approach? Do you think one of the tokenizations is better than another? Can you think of a way you would test which one is better? Refer to justification from the readings where appropriate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sZ7km6oRYZs"
      },
      "source": [
        "### Question 6A: Code (3/8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ld_dCKjLQaqf"
      },
      "outputs": [],
      "source": [
        "# select a slice of 5 abstracts from the documents\n",
        "abstracts_list_1 = abstracts [5:10]\n",
        "# print the length of this slice to show that it is five abstracts\n",
        "print(\"Length::\", len(abstracts_list_1))\n",
        "# Hint: Get the tokenizations from all 3 tokenization schemes by using the random indices in Hint 1\n",
        "# using split()\n",
        "split_slice =split_abstract [5:10]\n",
        "# using nltk\n",
        "nltk_slice = nltk_tokenized_abstracts [5:10]\n",
        "#using spacy\n",
        "spacy_slice = spacy_tokenized_abstract [5:10]\n",
        "# print the outputs of each of these 3 tokenizations for all 5 abstracts\n",
        "print(\"split_slice\" , split_slice)\n",
        "print(\"nltk_slice\" , nltk_slice)\n",
        "print(\"spacy_slice\", spacy_slice)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CbwIX8AQm9Y"
      },
      "source": [
        "### Question 6B: Free response (5/8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCVSKvDZGUDS"
      },
      "source": [
        "Using Split()\n",
        "\n",
        "**Strengths:**\n",
        "1. Here, you can seperate sentences on whitespaces. \n",
        "2. It is a simplest technique. Instead of whitespace you can use any delimiter. \n",
        "\n",
        "**Weakness:**\n",
        "1. It includes sentence ending punctuation with token. Instead of separating it from token.\n",
        "\n",
        "Using NLTK:\n",
        "\n",
        "NLTK is one of the most famous library in in Python.It can be used for various purposes like tokenizing, parsing,lemmatization, etc. \n",
        "\n",
        "**Strengths:**\n",
        "\n",
        "1. It supports many languages.\n",
        "\n",
        "2. Also, it handles text as a group of strings.\n",
        "\n",
        "**Weakness:**\n",
        "\n",
        "1. It does not support word vector.\n",
        "2. NLTK doesn't apply any semantic analysis on senetence tokenization.\n",
        "3. By using NLTK on large dataset leads to slow execution.\n",
        "\n",
        "Using spaCy:\n",
        "Instead of working on strings, spaCy handles all the data in the form of objects.\n",
        "\n",
        "\n",
        "**Strengths:**\n",
        "\n",
        "1. Perfomance wise it is better than NLTK.\n",
        "2. It supports word vectors.\n",
        "3. It is faster than NLTK.\n",
        "\n",
        "**Weakness:**\n",
        "\n",
        "1. It has less flexibility than NLTK.\n",
        "2. Compared to NLTK, it doesn't support many languages.\n",
        "\n",
        "According to me, tokenization using spaCy is better than other two techniques. As you can see in the result, spaCy divides the abstracts on brackets and on dashes also. Eventhough the output generated by both NLTK and spaCy is quite similar, still spaCy gives in depth information of the abstract by diving it into small small words. Using spaCy, we can analyze data in more efficient way.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlivQPJXf6m2"
      },
      "source": [
        "# Question 7: Tabulating word counts under different algorithms (8 points)\n",
        "\n",
        "Now that you have compared and contrasted different tokenization algorithms, consider the effect that tokenization can have on our ability to characterize a corpus as a whole. \n",
        "\n",
        "Load in the `Counter` module and extract counts of all of the words under each of the three tokenizations schemes. Look at the top 5 most frequent (using the `.most_frequent()` method) and the top 10 least frequent (hint: use negative indices) words. In our data, what appear to be the biggest sources of disagreement? Do these confirm or disconfirm your hypotheses in the previous question? How or how not? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjwPTSJA_txp"
      },
      "source": [
        "### Question 7A: Code (3/8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "aiXEsEaVdJTB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37abfc74-5cbf-4e84-8537-f33e131a58a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "split_abstract_most_common [('the', 162905), ('of', 115534), ('and', 100664), ('a', 83250), ('to', 79238)]\n",
            "split_abstract_least_common [('Sponsorship', 1), ('Equipment', 1), ('(Ronald', 1), ('Borden);', 1), ('26-27,', 1), ('Wisbey);', 1), ('{MIT}', 1), ('(Jonathan', 1), ('Allen);', 1), ('Bailey);', 1)]\n"
          ]
        }
      ],
      "source": [
        "#On split_abstract\n",
        "tuple1 = tuple(split_abstract)\n",
        "split_counter = Counter(chain(*tuple1))\n",
        "split_abstract_most_common = split_counter.most_common(5)\n",
        "print(\"split_abstract_most_common\", split_abstract_most_common)\n",
        "\n",
        "split_abstract_least_common = split_counter.most_common()[-10:]\n",
        "print(\"split_abstract_least_common\", split_abstract_least_common)\n",
        "#Counter(tuple1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cz3895ZGtwEl",
        "outputId": "6cd2d204-f6f2-460a-9d88-76d970a280dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nltk_abstract_most_common [('the', 163172), (',', 160068), ('.', 158239), ('of', 115654), ('and', 101170)]\n",
            "nltk_abstract_least_common [('Organizational', 1), ('Guyford', 1), ('Stever', 1), ('Buyers', 1), ('Dake', 1), ('Gaddy', 1), ('Sponsorship', 1), ('Borden', 1), ('26-27', 1), ('Wisbey', 1)]\n"
          ]
        }
      ],
      "source": [
        "# on nltk\n",
        "abstracts_list_count = ' '.join(abstracts)\n",
        "abstract_tokenized_into_words = {}\n",
        "\n",
        "abstract_tokenized_into_words = word_tokenize(abstracts_list_count)\n",
        "nltk_counter = Counter(abstract_tokenized_into_words)\n",
        "\n",
        "nltk_abstract_most_common = nltk_counter.most_common(5)\n",
        "print(\"nltk_abstract_most_common\", nltk_abstract_most_common)\n",
        "\n",
        "nltk_abstract_least_common = nltk_counter.most_common()[-10:]\n",
        "print(\"nltk_abstract_least_common\", nltk_abstract_least_common)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# on spacy\n",
        "flat=[]\n",
        "for i in spacy_tokenized_abstract:\n",
        "  for j in i:\n",
        "    flat.append(j)\n",
        "\n",
        "counter_method_output = Counter(flat)\n",
        "\n",
        "spacy_abstract_most_common = counter_method_output.most_common(5)\n",
        "print(\"spacy_abstract_most_common \", spacy_abstract_most_common)\n",
        "spacy_abstract_least_common = counter_method_output.most_common()[-10:]\n",
        "print(\"spacy_abstract_least_common\" , spacy_abstract_least_common)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwa0SMbl_OdE",
        "outputId": "6b60f4bb-67bd-41ae-b874-4c7e2b7f344c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spacy_abstract_most_common  [('the', 168208), (',', 159384), ('.', 158688), ('of', 121903), ('-', 104316)]\n",
            "spacy_abstract_least_common [('Shooman', 1), ('Organizational', 1), ('Guyford', 1), ('Stever', 1), ('Buyers', 1), ('Dake', 1), ('Gaddy', 1), ('Sponsorship', 1), ('Borden', 1), ('Wisbey', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bud1U9iM_vgh"
      },
      "source": [
        "### Question 7B: Free response (5/8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTWirBBEGPjO"
      },
      "source": [
        "In previous question, I said that spaCy is better than the other two orgnization. If you see the result of split_abstract_most_common, the abstracts are splitted on whitespace only. And that won't be helpful for any further advance analaysis. We can also see that, output genrated by NLTK and spaCy are almost similar. But spaCy tokenize words in great detail. For example, in NLTK, it considered '26-27' as a least frequent word, whereas in spaCy, it divides '26-27' into three parts '26' , '-' , '27' . So, I agree with the hypothesis that I made in previous question which is spaCy> NLTK > split()."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FvuYyre5F_P"
      },
      "source": [
        "# Question 8: Tabulating pointwise mutual information under different tokenization schemes: 8 points\n",
        "\n",
        "Mutual information is a computation that is very similar to computing a conditional probability. Recall that computing a conditional probability, defined below, requires knowing two probabilities. The first, $p(A \\cap B)$, is the probability of observing $A$ and $B$ at the same time. The second, $p(A)$, is the probability of observing $A$ across all contexts.\n",
        "\n",
        "Recall that we can approximate all of these by their frequencies in a corpus. For example, $p(A)$ can be approximated by:\n",
        "\n",
        "<center> $\\large p(A) \\approx \\frac{count(A)}{\\sum_{w \\in V}count(w)}$ </center>\n",
        "\n",
        "A conditional probability like $p(B | A)$ is a measure that allows us to estimate how many of our observations of $B$ occur having already seen $A$.\n",
        "\n",
        "<center>$\\large p(B | A) = \\frac{p(A \\cap B)}{p(A)}$</center>\n",
        "\n",
        "Mutual information is very similar, but requires dividing the co-occurence statistic by two probabilities $p(A)$ and $p(B)$.\n",
        "\n",
        "<center>$\\large MI = \\frac{p(A \\cap B)}{p(A) \\cdot p(B)}$</center>\n",
        "\n",
        "<hr />\n",
        "\n",
        "This question contains multiple parts to respond to.\n",
        "\n",
        "1. Compute the bigram frequencies of all words in our `abstracts.tsv` corpus. You may use whatever tokenization scheme you think performs the best.\n",
        "2. Pick one of your tokenized abstracts from Question 5 that you think sounds interesting.\n",
        "3. For each of the bigrams in that abstracts, compute the mutual information of that bigram and print the bigram and its mutual information value to the notebook.\n",
        "4. Answer the questions in the free response section.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pXgmepfDDAm"
      },
      "source": [
        "### Question 8A: Computing mutual information for bigrams in one sentence (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#code for getting the bigram pairs & the counts: \n",
        "flat_spacy_tokenized_abstract=[]\n",
        "for i in spacy_tokenized_abstract:\n",
        "  for j in i:\n",
        "    flat_spacy_tokenized_abstract.append(j)\n",
        "\n",
        "list_of_counts = {}\n",
        "iteration = 0\n",
        "length = len(flat_spacy_tokenized_abstract)\n",
        "\n",
        "for word in flat_spacy_tokenized_abstract:\n",
        "  if iteration != length-1:\n",
        "    if word not in list_of_counts:\n",
        "      list_of_counts[word] = {}\n",
        "    next_word = flat_spacy_tokenized_abstract[iteration+1]\n",
        "    if next_word in list_of_counts[word]:\n",
        "      list_of_counts[word][next_word] = list_of_counts[word][next_word] + 1\n",
        "    else:\n",
        "      list_of_counts[word][next_word] = 1\n",
        "  iteration = iteration + 1\n",
        "  \n",
        "#print(list_of_counts['language'])"
      ],
      "metadata": {
        "id": "la2UqASLpHUd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for calculating the probability & etc on abstract[5:10]: \n",
        "list_of_counts\n",
        "total_pairs = 0\n",
        "for key in list_of_counts:\n",
        "  total_pairs = total_pairs + sum(list_of_counts[key].values())\n",
        "  \n",
        "def calculations_1(word_a, word_b, choosen_abstract):\n",
        "  number_counts_a = choosen_abstract.count(word_a)\n",
        "  number_counts_b = choosen_abstract.count(word_b)\n",
        "  if word_b not in list_of_counts[word_a]:\n",
        "    number_counts_a_and_b = 0\n",
        "  else:\n",
        "    number_counts_a_and_b = list_of_counts[word_a][word_b]\n",
        "  total_words = len(choosen_abstract)\n",
        "\n",
        "  prob_a = number_counts_a/total_words\n",
        "  prob_b = number_counts_b/total_words\n",
        "  prob_a_inter_b = number_counts_a_and_b/total_pairs\n",
        "  cond_prob = prob_a_inter_b/(prob_a)\n",
        "  MI = prob_a_inter_b/(prob_a*prob_b)\n",
        "\n",
        "  print(word_a + ' = ' + str(number_counts_a))\n",
        "  print(word_b + ' = ' + str(number_counts_b))\n",
        "  print('number of counts = ' + str(number_counts_a_and_b))\n",
        "#  print('total_pairs = ' + str(total_pairs))\n",
        "#  print('prob_a = ' + str(prob_a))\n",
        "#  print('prob_b = ' + str(prob_b))\n",
        "# print('prob_a_inter_b = ' + str(prob_a_inter_b))\n",
        "#print('cond_prob = ' + str(cond_prob))\n",
        "  print('MI = ' + str(MI))"
      ],
      "metadata": {
        "id": "9rksZo4wE3FR"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for calculating probability on abstracts[5:10]:\n",
        "flat_spacy_tokenized_abstract=[]\n",
        "for i in spacy_tokenized_abstract[5:10]:\n",
        "  for j in i:\n",
        "    flat_spacy_tokenized_abstract.append(j)\n",
        "\n",
        "iter = 0\n",
        "length  = len(flat_spacy_tokenized_abstract)\n",
        "for word_a in flat_spacy_tokenized_abstract:\n",
        "  for word_b in flat_spacy_tokenized_abstract[iter+1:length]:\n",
        "    calculations_1(word_a, word_b, flat_spacy_tokenized_abstract)"
      ],
      "metadata": {
        "id": "3igawtW-pgtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below code, is finding MI for all abstracts."
      ],
      "metadata": {
        "id": "GgEfjFoyZxZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. or calculating probability & MI on entire abstract:\n",
        "\n",
        "#COND_PROB_LIST = []\n",
        "#flat_spacy_tokenized_abstract=[]\n",
        "#for i in spacy_tokenized_abstract:\n",
        "#  for j in i:\n",
        "#    flat_spacy_tokenized_abstract.append(j)\n",
        "\n",
        "#calculations('parsing', 'algorithm' , flat_spacy_tokenized_abstract)\n",
        "flat_spacy_tokenized_abstract=[]\n",
        "for i in spacy_tokenized_abstract:\n",
        "  for j in i:\n",
        "    flat_spacy_tokenized_abstract.append(j)\n",
        "\n",
        "iter = 0\n",
        "length  = len(flat_spacy_tokenized_abstract)\n",
        "for word_a in flat_spacy_tokenized_abstract:\n",
        "  for word_b in flat_spacy_tokenized_abstract[iter+1:length]:\n",
        "    calculations(word_a, word_b, flat_spacy_tokenized_abstract)"
      ],
      "metadata": {
        "id": "jYU7cSpCpnMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for calculating the probability & etc on entire abstract data: \n",
        "list_of_counts\n",
        "total_pairs = 0\n",
        "for key in list_of_counts:\n",
        "  total_pairs = total_pairs + sum(list_of_counts[key].values())\n",
        "  \n",
        "def calculations(word_a, word_b, choosen_abstract):\n",
        "  number_counts_a = choosen_abstract.count(word_a)\n",
        "  number_counts_b = choosen_abstract.count(word_b)\n",
        "  if word_b not in list_of_counts[word_a]:\n",
        "    number_counts_a_and_b = 0\n",
        "  else:\n",
        "    number_counts_a_and_b = list_of_counts[word_a][word_b]\n",
        "  total_words = len(choosen_abstract)\n",
        "\n",
        "  prob_a = number_counts_a/total_words\n",
        "  prob_b = number_counts_b/total_words\n",
        "  prob_a_inter_b = number_counts_a_and_b/total_pairs\n",
        "  #cond_prob = prob_a_inter_b/(prob_a)\n",
        "  MI = prob_a_inter_b/(prob_a*prob_b)\n",
        "\n",
        "  #print(word_a + ' = ' + str(number_counts_a))\n",
        "  #print(word_b + ' = ' + str(number_counts_b))\n",
        "  #print('number of counts = ' + str(number_counts_a_and_b))\n",
        "  #print('total_pairs = ' + str(total_pairs))\n",
        "  #print('prob_a = ' + str(prob_a))\n",
        "  #print('prob_b = ' + str(prob_b))\n",
        "  #print('prob_a_inter_b = ' + str(prob_a_inter_b))\n",
        "  #print('cond_prob = ' + str(cond_prob))\n",
        "  #print('MI = ' + str(MI))"
      ],
      "metadata": {
        "id": "V_MmAXDMruMu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIEw6FcsdZ1f"
      },
      "source": [
        "### Question 8B: Free response (3 points)\n",
        "\n",
        "Characterize the different mutual information values of the sentence you used. What values are highest? What values are lowest? When do you think mutual information would be a better statistic to compute than a conditional probability?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzshSnbtGMCo"
      },
      "source": [
        "\n",
        "Mutual information is nothing but the mutual dependance between two variables. whereas conditional probability is the probabilty of an event occurring given that another event has already occurred. As you can see in the result, some of the biagrams have high mutual information. And, If the mutual information is high then it means there is large reduction in uncertainty. And that's what we want, so that we can get better idea about text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCgDQrOaICRl"
      },
      "source": [
        "# Submission guidelines (1 point)\n",
        "\n",
        "Please upload your completed notebook file to UBLearns in the following format:\n",
        "\n",
        "Lastname\\_Firstname\\_HW2.ipynb\n",
        "\n",
        "e.g., Smith\\_John\\_HW2.ipynb."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ccD5Kq24WAN3",
        "SZSPSo-TWTxL",
        "LRL5OkT4Wn-4",
        "2UqHOFeVIGzB",
        "9PpfhdWSI0Me",
        "4sZ7km6oRYZs",
        "0CbwIX8AQm9Y",
        "WjwPTSJA_txp",
        "EzshSnbtGMCo",
        "PCgDQrOaICRl"
      ],
      "name": "Vichare_Vaibhavi_HW2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}