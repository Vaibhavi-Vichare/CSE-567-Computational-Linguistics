{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Homework_4.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"HHR5JHLfveqC"},"source":["# Dataset exploration\n","\n","This assignment is designed to help you get a sense of the raw data we often work with as researchers. For this assignment, you will be using `pandas` in addition to your own knowledge of linguistic structure from the readings. This assignment requires the qualitative and quantitative assessment of several parts of a very small corpus. \n","\n","The coding components of this assignment will largely require loading in the data and building a simple input system that allows you to classify two instances of the same string as they appear in different sentences as meaning the same or different things. \n","\n","The assignment is designed to give you a feel for what we want out of computational models of semantic representations of words and also give you experience doing annotation for NLP tasks."]},{"cell_type":"markdown","metadata":{"id":"_-214d0dvukt"},"source":["# The problem: Ambiguous words\n","\n","Language is often ambiguous, with the same string or series of sounds corresponding to different meanings. This **ambiguity** leads to challenges for NLP, such as distinguishing between seriousness and sarcasm (e.g., \"That was great\"), or between completely different, unrelated meanings (e.g., \"I couldn't bear to part with it\" vs. \"The bear did not want to part with it\"). Often, we are interested in identifying what meaning a word has in context. Can we properly guess what a speaker means by \"bear\", or \"great\"?\n","\n","One of the major areas of natural language processing is **Word Sense Disambiguation** (WSD). Under these schemes, a string is assumed to have several possible **senses**, or distinct meanings. We will discuss in greater detail in class why the idea that words have distinct meanings is challenging, from identifying and differentiating these meanings to building models that can assign a label to a given instance of a word. \n","\n","## The dataset\n","\n","This assignment will center around annotating a dataset of sentences that are stored in a `.tsv` file. This dataset, like the others we have worked with, contains one \"document\" per line. Each document in this dataset corresponds to one line, or subtitle, from a corpus of English subtitles for movies, along with some **metadata**. This particular file is structured as a matrix where the rows are records and the columns are values for that record -- it is very similar to a dictionary structure.\n","\n","The subtitles are presented out of context, one on each line in the `.tsv`.\n","\n","Some example subtitles, in the first column of the dataframe (`\"subtitle\"`) will look like this:\n","\n","* \"Neither his post , nor adjacent .\"\n","* \"Send men out to post a guard .\"\n","\n","In the column `'word'` you will find **ambiguous words**. We have selected a small number of ambiguous words for you that have a diverse range of meanings for each. \n","\n","The dataset itself comes from [Rice et al. (2018)](https://link.springer.com/article/10.3758/s13428-018-1107-7), a study that looked at how common different types of ambiguous words were in a corpus of subtitles. In the third column in the tsv is the `'meaning_label'`. This is a categorical label (a string `str`) for meanings from an internet dictionary called Wordsmyth. The values are often represented as numbers (\"1\", \"2\"), but may also be represented as a dash character \"-\" to indicate something like, this meaning is \"not in the Wordsmyth list.\" The labels themselves have been defined somewhat informally by lexicographers. The label you see was the product of two separate annotators (students in the Rice et al. paper's lab) going through each of these sentences by hand and assigning labels to them with the help of the dictionary.\n","\n","Here we will be doing something similar, but less complex."]},{"cell_type":"markdown","metadata":{"id":"u23wQRbv1Yfs"},"source":["# Question 0: Your imports (1 point)"]},{"cell_type":"code","metadata":{"id":"QfyBL7Q91btk"},"source":["# put all of your imports here\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dHnErlZKu88I"},"source":["# Question 1: Loading in data using pandas (1 point)\n","\n","Import your code below and load the file `ambiguous.tsv` into the notebook. Convert it into a dataframe using `pandas`. Your dataframe should be called `df`. The format for loading in the dataframe will look like this:\n","\n","```python\n","import pandas as pd\n","\n","df = pd.read_csv(\"/path/to/my/filename_for_this_assignment.tsv\", sep=\"\\t\")\n","```\n","\n","The code for this will be a little more complicated if you are loading it in through `files.upload()`:\n","\n","```python\n","import pandas as pd\n","from io import BytesIO\n","\n","my_file = files.upload()\n","\n","df = pd.read_csv(BytesIO(my_file[\"filename_for_this_assignment.tsv\"]), sep=\"\\t\")\n","```"]},{"cell_type":"code","metadata":{"id":"T8cW5GlutqTQ"},"source":["# load in the document and store it as the desired dataframe"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XiBvkUqX0Zn6"},"source":["# Question 2: Understanding the dataframe (3 points)\n","\n","Now that you have `df` in the file, please **print** the answer to each of the following. If you have not worked with `pandas` before, please refer to the [documentation](https://pandas.pydata.org/docs/) and/or search for the following on Stack Overflow. If you search for these online, be sure to include the term \"pandas\" in your search. Each response is worth 1 point.\n","\n","* The number of rows in `df`\n","* The names of the columns in `df`\n","* The unique values in `df['word']`"]},{"cell_type":"code","metadata":{"id":"enTEqwKh0kHp"},"source":["# number of rows\n","\n","# column names of df\n","\n","# unique values in df['word']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IJhTEOA71SNO"},"source":["# Question 3: Subsetting the data (5 points)\n","\n","For this question, I would like you to subset to the rows in the dataframe that only have the word \"post\" in them. In order to do this, refer to the `pandas` subsetting documentation: https://pandas.pydata.org/docs/getting_started/intro_tutorials/03_subset_data.html\n","\n","As with most comparisons, you will want to make sure you use the `==` operator from previous assignments. So, be sure to subset to all rows where the string is `\"post\"`. \n","\n","Save the subset of the data as a variable called `post_df`."]},{"cell_type":"markdown","metadata":{"id":"pZFoKMy4-Oh2"},"source":["### Question 3A: Subsetting to a single matching string (2 points)"]},{"cell_type":"code","metadata":{"id":"leGUlYLU1Rnn"},"source":["# your code for creating the post dataframe goes here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9agn2yuY-GO_"},"source":["## Question 3B: Describe the subset (1 point)\n","\n","Once you have created your `post_df` dataframe, print out the length of `post_df` in terms of the number of rows."]},{"cell_type":"code","metadata":{"id":"FJx720uPTrOb"},"source":["# print the number of rows of post_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5dDc5ObkTMuC"},"source":["## Question 3C: Create a subset for sense \"2\" in post_df\n","\n","Now, create an additional subset to get the \"2\" sense. Store this smaller subset as `two_post_df`. Print the number of rows in `two_post_df`."]},{"cell_type":"code","metadata":{"id":"YfIYcKwUTzhY"},"source":["# create smaller subset\n","\n","# print number of rows"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hvdlCV1_9502"},"source":["# Question 4: Data Annotation (21 points total)\n","\n","This question leads you through a long journey to understand how we can get data for our machine learning models. Below, you will find functions that will help us annotate our data inside a notebook. We can store our annotations and then use them to label our data later. \n","\n","For this annotation task, we will be simplifying the task of Word Sense Disambiguation down to pairwise comparisons. You will compare randomly selected sentences that contain two instances of the same string (i.e., \"post\"). We want you to annotate whether the use of \"post\" means roughly the same or different things in the sentences. This task is challenging because sometimes it is unclear from a sentence or there is not enough information to go on. Try your best and answer the questions below. You may have to go through the data several times, but it should teach you a lot about what we need to do good word sense disambiguation!"]},{"cell_type":"markdown","metadata":{"id":"OS-hvZVzVTI4"},"source":["## Question 4A: Run and answer the prompt (1 point)"]},{"cell_type":"code","metadata":{"id":"lZii1xVS98Cj"},"source":["# Run the below code and complete one example\n","\n","def subtitle_randomizer(subset_df):\n","  # get two rows\n","  two_rows = subset_df.sample(n=2, replace=False)\n","  sent_one, sent_two = two_rows['subtitle'].tolist()\n","  sense_sent_one, sense_sent_two = two_rows['meaning_label'].tolist()\n","  senses_match_flag = sense_sent_one==sense_sent_two\n","  print(f\"First sentence: {sent_one}\\nSecond sentence: {sent_two}\")\n","  return senses_match_flag\n","\n","def request_judgment():\n","  answer = ''\n","  while answer not in {'same', 'different'}:\n","    answer = input(\"Is the meaning of 'post' in these two sentences the same or different?         :    \")\n","  if answer == 'same':\n","    same_flag = True\n","  elif answer== 'different':\n","    same_flag = False\n","  return same_flag\n","\n","def annotate_one(subset_df):\n","  gold_label = subtitle_randomizer(subset_df)\n","  annotator_says_match_flag = request_judgment()\n","  print(f\"\\nGold label: {gold_label}\\nAnnotator answer: {annotator_says_match_flag}\\n\")\n","  return gold_label, annotator_says_match_flag\n","\n","gold_label, annotator_match = annotate_one(post_df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N2y6tY76YOV1"},"source":["## Question 4B: Reflection on 4A (2 points)\n","\n","For the specific example you saw above, elaborate on **how you made your decision** about whether the meanings were the same or different. Go into detail on the specific example -- **be specific**. Were there any **difficulties** that the sentences posed? If it was easy, what kind of information did you use? Is there a way that you can think of that would **make the task easier**? Refer to the readings from Bender and Lascarides (Chapter 4) and SLP3 (Chapter 18) where appropriate."]},{"cell_type":"markdown","metadata":{"id":"kvH6iBifYQAr"},"source":["<font color=\"red\"> Your answer to question 4B goes here. You may delete this line when you have provided your answer.</font>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jqZl8ot0Xqwt"},"source":["## Question 4C: Randomly sample multiple pairs (8 points)\n","\n","Create a new list called `accuracies`.\n","\n","Randomly sample `post_df` 30 times using a `for` loop. For this question, you will make judgments about whether the senses in the two sentences you get are the same or not. To say whether two senses are the same, you should write \"same\" (without quotes) in the text box. If the two senses are different, you should write \"different\" (without quotes). **Keep track of what makes your judgments difficult** for Question 4C. Try to do your very best on these classifications and go through the answers until you get about 80% of these correct.\n","\n","For each answer you make, you should **compare whether your answer matched** the `gold_label`. So, inside your loop, create a variable (a boolean) called `correct` that is whether your answer and the gold label match. Refer to the code in the `subtitle_randomizer` function if necessary."]},{"cell_type":"code","metadata":{"id":"inNlsQ50XlxV"},"source":["# Create a list to store the accuracy of your responses\n","\n","\n","# Create a loop to iteratively compare 30 pairs of sentences\n","  ## - Use the annotate_one function with post_df\n","\n","  ## check whether your answer is correct\n","\n","  ## add the correctness score for your answer to accuracy list\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0cHgrkHoSEeo"},"source":["## Question 4D: Assessing your accuracy (2 points)\n","\n","Now, take the average of `accuracies` using `np.mean` and save it into a variable called `mean_accuracy`. Report this value by printing it. \n","\n","#### Note: If your mean_accuracy is under .8, please go back up to 4C and rerun. Keep track of the number of times you have run."]},{"cell_type":"code","metadata":{"id":"FNeZzOc5Zeeh"},"source":["# compute the proportion of your responses that were correct\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3qeYeuy2Zib2"},"source":["## Question 4E: Reflection on multiple rounds of annotation (8 points)\n","\n","We asked you to assess your accuracy in 4D. Recall that in 4A we asked you to try to do your best. Tell us a little bit about the following:\n","\n","1. About how long did it take you to get to 80% accuracy on the annotation?\n","2. What made some annotations easy?\n","3. What made some annotations hard?\n","4. Name some specific examples of sentences you got wrong and why you think that happened.\n","Refer to the readings from Bender and Lascarides (Chapters 3 and 4) and SLP3 (Chapter 18) where appropriate."]},{"cell_type":"markdown","metadata":{"id":"v03LkIErSH9z"},"source":["<font color=\"red\" /> Your answer to question 4E goes here. You may delete this line when you have provided your answer.</font>"]},{"cell_type":"markdown","metadata":{"id":"nMXOOFm7jb7v"},"source":["# Bonus (Free response: 5 points)\n","\n","Describe another way of solving the problem of annotating senses. Flesh out the procedure you would use, and explain why you think it would be better than the way we did the task here. What improvements does your task make upon the task in Questions 0-4? Given what we have discussed about senses and the difficulty in marking clear boundaries with them, does your method solve this problem? Why or why not?"]},{"cell_type":"markdown","metadata":{"id":"arWYk7cAjtL2"},"source":["<font color=\"red\" /> Your answer to the Bonus goes here. You may delete this line when you have answered it.</font>"]}]}